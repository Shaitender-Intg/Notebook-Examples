{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"/home/shaitender/data/libor-samples/\"\n",
    "txt_files = os.path.join(DIR_PATH, '*.txt')\n",
    "\n",
    "DIR_PATH_OUTPUT = \"/home/shaitender/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a13ce4683d063459740ae578b25bf5193db3a97.txt\n",
      "0a16dac27077c6cbc59daa1d79fb481d315144e1.txt\n",
      "00ce2984594a221379ee1b5d57b573318c0cf769.txt\n",
      "0a9bed39f77ffe204e22a9e22e115959961552b1.txt\n",
      "0a9f45ec4e6520de9a54b0865ef9f54f7685b4ce.txt\n",
      "00c558fb64b1e20cf1e3ee55de0aa1a682d9aac3.txt\n",
      "0a68f54cbf6e69cf302ae59f5442ebc8590a1555.txt\n",
      "00c06ea21707e73a15f5e5ab912a4e435aa6fce0.txt\n",
      "0a4b30ed0274be26eb44eaa262a931af24151cf7.txt\n",
      "000e28022641ffb22920352c207f5280f9dcc710.txt\n",
      "00c8143f173fc3a0f5d5de9e36bd27083887abbc.txt\n",
      "00b41c45c3a3b1d660543340b1a2f47f4bdd78e2.txt\n",
      "00caf466cf572d564b0b219f7bd3f3278842daef.txt\n",
      "0a58d5447a6bd375980a19f00ae12a5f06e58249.txt\n",
      "0a96e94f6cc4ab4b68d5b7537255c963cdd0ee63.txt\n",
      "00ea37063bcdcafaeb1177466a5977f7ee0206dc.txt\n",
      "0a026fa8a00a66a33c0875cd06c91d81f154fce6.txt\n",
      "00cecaea5efb47865850e6be3bf94589acdf23e4.txt\n",
      "0a84b30cb519d58f1f71ebff9832c254fd875b65.txt\n",
      "0a0c6f140b3e877598680c4367ec871bee72bd8a.txt\n",
      "0a61f6a11160966394b3af1e5efc5e6c25ccde1a.txt\n"
     ]
    }
   ],
   "source": [
    "output = DIR_PATH_OUTPUT+'output_file.csv'\n",
    "\n",
    "with open(output, 'w') as outfile:\n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(['FileName', 'Content'])\n",
    "\n",
    "    files = os.listdir(DIR_PATH)\n",
    "\n",
    "    for filename in files:\n",
    "        with open(DIR_PATH + filename) as afile:\n",
    "            csvout.writerow([filename, afile.read()])\n",
    "            afile.close()\n",
    "            print(filename)\n",
    "\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0a13ce4683d063459740ae578b25bf5193db3a97.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nRevolving Credit Agreement\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a16dac27077c6cbc59daa1d79fb481d315144e1.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nPress release - signing of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00ce2984594a221379ee1b5d57b573318c0cf769.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nCredit Agreement, dated November...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a9bed39f77ffe204e22a9e22e115959961552b1.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nWaiver and Second Amendment to C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a9f45ec4e6520de9a54b0865ef9f54f7685b4ce.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nExhibit 10.18\\n\\n\\n\\nEXECUTION V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00c558fb64b1e20cf1e3ee55de0aa1a682d9aac3.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nexv10w1\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0a68f54cbf6e69cf302ae59f5442ebc8590a1555.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nWaiver and First Amendment to Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00c06ea21707e73a15f5e5ab912a4e435aa6fce0.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nFifth Amendment to the Senior Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0a4b30ed0274be26eb44eaa262a931af24151cf7.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000e28022641ffb22920352c207f5280f9dcc710.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nSecond Amendment to Credit Agree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00c8143f173fc3a0f5d5de9e36bd27083887abbc.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nFY2014_Q2_8K_Ex10.2_Five Year Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00b41c45c3a3b1d660543340b1a2f47f4bdd78e2.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nCredit Agmt\\n\\n\\n\\n Exhibit 10.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00caf466cf572d564b0b219f7bd3f3278842daef.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nexhibit10.htm\\n\\n\\nExhibit 10.1\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0a58d5447a6bd375980a19f00ae12a5f06e58249.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nex10-1.htm\\n\\n\\n\\nExhibit 10.1\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0a96e94f6cc4ab4b68d5b7537255c963cdd0ee63.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nex10.htm\\n\\n\\n    \\n      \\n\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00ea37063bcdcafaeb1177466a5977f7ee0206dc.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nExhibit\\n\\n\\n\\nExhibit 10.16\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0a026fa8a00a66a33c0875cd06c91d81f154fce6.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nAMENDED AND RESTATED CREDIT AGRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00cecaea5efb47865850e6be3bf94589acdf23e4.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nCredit Agreement dated as of Feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0a84b30cb519d58f1f71ebff9832c254fd875b65.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nCredit Agreement\\n\\n\\n\\n Exhibit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0a0c6f140b3e877598680c4367ec871bee72bd8a.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExhibit 10.1\\n\\n\\n\\nTH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0a61f6a11160966394b3af1e5efc5e6c25ccde1a.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nLIMITED WAIVER AND AMENDMEMT #17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        FileName  \\\n",
       "0   0a13ce4683d063459740ae578b25bf5193db3a97.txt   \n",
       "1   0a16dac27077c6cbc59daa1d79fb481d315144e1.txt   \n",
       "2   00ce2984594a221379ee1b5d57b573318c0cf769.txt   \n",
       "3   0a9bed39f77ffe204e22a9e22e115959961552b1.txt   \n",
       "4   0a9f45ec4e6520de9a54b0865ef9f54f7685b4ce.txt   \n",
       "5   00c558fb64b1e20cf1e3ee55de0aa1a682d9aac3.txt   \n",
       "6   0a68f54cbf6e69cf302ae59f5442ebc8590a1555.txt   \n",
       "7   00c06ea21707e73a15f5e5ab912a4e435aa6fce0.txt   \n",
       "8   0a4b30ed0274be26eb44eaa262a931af24151cf7.txt   \n",
       "9   000e28022641ffb22920352c207f5280f9dcc710.txt   \n",
       "10  00c8143f173fc3a0f5d5de9e36bd27083887abbc.txt   \n",
       "11  00b41c45c3a3b1d660543340b1a2f47f4bdd78e2.txt   \n",
       "12  00caf466cf572d564b0b219f7bd3f3278842daef.txt   \n",
       "13  0a58d5447a6bd375980a19f00ae12a5f06e58249.txt   \n",
       "14  0a96e94f6cc4ab4b68d5b7537255c963cdd0ee63.txt   \n",
       "15  00ea37063bcdcafaeb1177466a5977f7ee0206dc.txt   \n",
       "16  0a026fa8a00a66a33c0875cd06c91d81f154fce6.txt   \n",
       "17  00cecaea5efb47865850e6be3bf94589acdf23e4.txt   \n",
       "18  0a84b30cb519d58f1f71ebff9832c254fd875b65.txt   \n",
       "19  0a0c6f140b3e877598680c4367ec871bee72bd8a.txt   \n",
       "20  0a61f6a11160966394b3af1e5efc5e6c25ccde1a.txt   \n",
       "\n",
       "                                              Content  \n",
       "0   \\n\\n\\n\\n\\n\\n\\nRevolving Credit Agreement\\n\\n\\n...  \n",
       "1   \\n\\n\\n\\n\\n\\n\\nPress release - signing of the C...  \n",
       "2   \\n\\n\\n\\n\\n\\n\\nCredit Agreement, dated November...  \n",
       "3   \\n\\n\\n\\n\\n\\n\\nWaiver and Second Amendment to C...  \n",
       "4   \\n\\n\\n\\n\\n\\n\\nExhibit 10.18\\n\\n\\n\\nEXECUTION V...  \n",
       "5   \\n\\n\\n\\n\\n\\n\\nexv10w1\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n    ...  \n",
       "6   \\n\\n\\n\\n\\n\\n\\nWaiver and First Amendment to Cr...  \n",
       "7   \\n\\n\\n\\n\\n\\n\\nFifth Amendment to the Senior Se...  \n",
       "8   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n\\n          ...  \n",
       "9   \\n\\n\\n\\n\\n\\n\\nSecond Amendment to Credit Agree...  \n",
       "10  \\n\\n\\n\\n\\n\\n\\nFY2014_Q2_8K_Ex10.2_Five Year Cr...  \n",
       "11  \\n\\n\\n\\n\\n\\n\\nCredit Agmt\\n\\n\\n\\n Exhibit 10.1...  \n",
       "12  \\n\\n\\n\\n\\n\\n\\nexhibit10.htm\\n\\n\\nExhibit 10.1\\...  \n",
       "13  \\n\\n\\n\\n\\n\\n\\nex10-1.htm\\n\\n\\n\\nExhibit 10.1\\n...  \n",
       "14  \\n\\n\\n\\n\\n\\n\\nex10.htm\\n\\n\\n    \\n      \\n\\n  ...  \n",
       "15  \\n\\n\\n\\n\\n\\n\\nExhibit\\n\\n\\n\\nExhibit 10.16\\n\\n...  \n",
       "16  \\n\\n\\n\\n\\n\\n\\nAMENDED AND RESTATED CREDIT AGRE...  \n",
       "17  \\n\\n\\n\\n\\n\\n\\nCredit Agreement dated as of Feb...  \n",
       "18  \\n\\n\\n\\n\\n\\n\\nCredit Agreement\\n\\n\\n\\n Exhibit...  \n",
       "19  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExhibit 10.1\\n\\n\\n\\nTH...  \n",
       "20  \\n\\n\\n\\n\\n\\n\\nLIMITED WAIVER AND AMENDMEMT #17...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df['Content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = []\n",
    "for s in df['Content']:\n",
    "  sentences.append(sent_tokenize(s))\n",
    "\n",
    "sentences = [y for x in sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\n\\n\\nRevolving Credit Agreement\\n\\n\\n\\n \\xa0\\n Exhibit 10.1\\n\\n Execution Version \\n REVOLVING CREDIT AGREEMENT \\n (180-Day Facility) \\n\\ndated as of October\\xa015, 2010 \\n among \\n ATMOS ENERGY CORPORATION, \\n\\nas Borrower, \\n\\nTHE LENDERS FROM TIME TO TIME PARTY HERETO, \\n SUNTRUST BANK, \\n as Administrative Agent, \\n\\nWELLS FARGO BANK, N.A.',\n",
       " 'as Syndication Agent, \\n U.S. BANK NATIONAL ASSOCIATION \\n\\nand \\n\\nBANK OF AMERICA, N.A.',\n",
       " 'as Co-Documentation Agents \\n \\xa0\\n\\n\\xa0\\n \\xa0\\n\\nSUNTRUST ROBINSON HUMPHREY, INC. \\n As Sole Lead Arranger \\n and \\n\\nSUNTRUST ROBINSON HUMPHREY, INC., WELLS FARGO SECURITIES, LLC, \\n\\nU.S. BANK NATIONAL ASSOCIATION and BANC OF AMERICA SECURITIES, LLC \\n\\nas Joint-Bookrunners \\n\\n\\n\\n\\n\\n\\n\\n \\xa0\\n TABLE OF CONTENTS\\n\\n \\xa0\\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\xa0\\t\\xa0\\xa0\\t\\xa0\\t\\xa0\\xa0\\tPage\\t\\xa0\\n\\tARTICLE I DEFINITIONS; CONSTRUCTION\\t\\xa0\\xa0\\t\\xa0\\t1\\t\\xa0\\xa0\\n\\t Section\\xa01.1.',\n",
       " 'Definitions\\t\\xa0\\xa0\\t\\xa0\\t1\\t\\xa0\\xa0\\n\\t Section\\xa01.2.',\n",
       " 'Classifications of Loans and Borrowings\\t\\xa0\\xa0\\t\\xa0\\t14\\t\\xa0\\xa0\\n\\t Section\\xa01.3.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-01 07:02:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-07-01 07:02:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2019-07-01 07:02:18--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip          4%[                    ]  36.73M   298KB/s    in 3m 22s  \n",
      "\n",
      "2019-07-01 07:05:42 (186 KB/s) - Connection closed at byte 38515938. Retrying.\n",
      "\n",
      "--2019-07-01 07:05:43--  (try: 2)  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 862182613 (822M), 823666675 (786M) remaining [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M   747KB/s    in 20m 6s  \n",
      "\n",
      "2019-07-01 07:25:50 (667 KB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11140"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6580349807bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0msim_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1412\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;31m# no dtype conversion required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):    \n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_summaries = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 5 summaries\n",
    "for i in range(5):\n",
    "    print(ranked_summaries[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
